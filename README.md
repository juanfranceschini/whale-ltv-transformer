# ğŸ‹ Whale LTV Transformer

> **Early Customer Lifetime Value Prediction and Whale Identification using Transformer Architecture**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“– Overview

The Whale LTV Transformer is an advanced machine learning model that predicts customer lifetime value (LTV) and identifies high-value customers ("whales") from early customer behavior patterns. Built on the Brazilian E-Commerce (Olist) dataset, this model uses a Transformer architecture to capture sequential patterns in customer purchasing behavior.

### ğŸ¯ Key Features

- **Early LTV Prediction**: Predict 90-day customer value from first 14 days of behavior
- **Whale Identification**: Identify top 5% high-value customers with 93.6% accuracy
- **Sequential Pattern Recognition**: Transformer architecture captures temporal dependencies
- **Joint Training**: Simultaneous LTV regression and whale classification
- **Production Ready**: PyTorch Lightning framework with comprehensive testing

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/juanfranceschini/whale-ltv-transformer.git
cd whale-ltv-transformer

# Option 1: Install as a package (recommended)
pip install -e .

# Option 2: Install dependencies only
pip install -r requirements.txt
```

### Data Setup

```bash
# Download the Olist dataset from Kaggle and place CSV files in data/raw/
# Then prepare the data:
python -m src.data_prep
```

### Training

```bash
# Train the model
python -m src.train

# Or use the console script (if installed as package)
whale-ltv-train
```

### Quick Demo

```bash
# Run the example script to verify everything works
python example_usage.py
```

### Evaluation

```bash
# Run evaluation
python evaluate_model.py

# Or open the Jupyter notebook
jupyter notebook notebooks/whale_ltv_transformer_report.ipynb
```

## ğŸ“ Project Structure

```
whale-ltv-transformer/
â”œâ”€â”€ src/                    # Main source code
â”‚   â”œâ”€â”€ __init__.py        # Package exports
â”‚   â”œâ”€â”€ data_prep.py       # Data processing
â”‚   â”œâ”€â”€ train.py           # Training script
â”‚   â”œâ”€â”€ utils.py           # Utility functions
â”‚   â””â”€â”€ models/            # Model implementations
â”‚       â”œâ”€â”€ __init__.py    # Model exports
â”‚       â”œâ”€â”€ transformer.py # Transformer model
â”‚       â”œâ”€â”€ datamodule.py  # PyTorch Lightning data module
â”‚       â””â”€â”€ baselines.py   # Baseline models
â”œâ”€â”€ configs/               # Configuration files
â”œâ”€â”€ data/                  # Data directory
â”‚   â”œâ”€â”€ raw/              # Original Olist dataset
â”‚   â””â”€â”€ processed/        # Processed data
â”œâ”€â”€ notebooks/            # Jupyter notebooks
â”œâ”€â”€ tests/               # Unit tests
â”œâ”€â”€ scripts/             # Utility scripts
â”œâ”€â”€ models/              # Trained models (created during training)
â”œâ”€â”€ setup.py             # Package setup
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ example_usage.py     # Example script
â”œâ”€â”€ INSTALLATION.md      # Detailed installation guide
â””â”€â”€ README.md           # Project documentation
```

## ğŸ—ï¸ Model Architecture

### Core Components

1. **Event Embedding Layer**: Embeds order value, temporal, and rank features
2. **Transformer Encoder**: Multi-head attention with 4 layers and 8 attention heads
3. **Joint Output Heads**: Simultaneous LTV regression and whale classification
4. **Customer Feature Integration**: Combines sequence and customer-level features

### Key Features

- **Sequential Processing**: Captures temporal patterns in customer behavior
- **Attention Mechanism**: Provides interpretability through attention weights
- **Multi-task Learning**: Joint optimization improves both tasks
- **Scalable Design**: Handles variable-length sequences efficiently

## ğŸ“Š Dataset

### Olist Brazilian E-Commerce Dataset

- **Source**: [Kaggle Olist Dataset](https://www.kaggle.com/olistbr/brazilian-ecommerce)
- **Size**: ~100K customers, ~1M orders
- **Features**: Order history, customer demographics, product categories
- **Target**: 90-day customer lifetime value

### Data Processing

1. **Order Aggregation**: Calculate total order values (price + freight)
2. **Sequence Creation**: Create customer order sequences with temporal features
3. **Feature Engineering**: Extract customer-level features from sequences
4. **Target Calculation**: Compute 90-day LTV and whale classification

## ğŸ› ï¸ Development

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src

# Run specific test file
pytest tests/test_transformer.py
```

### Code Quality

```bash
# Format code
black src/ tests/

# Lint code
flake8 src/ tests/

# Type checking
mypy src/
```

## ğŸ“Š Evaluation Notebooks

- **[Comprehensive Evaluation](notebooks/whale_ltv_evaluation.ipynb)** - Complete model analysis and visualization
- **[Baseline Comparison](notebooks/baseline_comparison.ipynb)** - Model vs. traditional ML approaches

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Set up pre-commit hooks
pre-commit install
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Contact

- **Project Link**: [https://github.com/juanfranceschini/whale-ltv-transformer](https://github.com/juanfranceschini/whale-ltv-transformer)

---

**â­ Star this repository if you find it useful!**

*Open Source â¤ï¸* 