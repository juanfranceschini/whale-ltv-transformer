# Whale LTV Transformer Configuration

# Random seed for reproducibility
seed: 42

# Output directory
output_dir: "models"

# Data configuration
data:
  data_path: "data/processed/customer_sequences.parquet"
  batch_size: 32
  num_workers: 4
  max_sequence_length: 10
  test_size: 0.2
  val_size: 0.1
  normalize_ltv: true

# Model configuration
model:
  d_model: 128
  nhead: 8
  num_layers: 2
  dim_feedforward: 512
  dropout: 0.1
  ltv_weight: 0.6  # Weight for LTV loss in joint loss

# Training configuration
training:
  max_epochs: 50
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 10
  accelerator: "auto"  # "cpu", "gpu", "auto"
  devices: 1
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  val_check_interval: 1.0
  deterministic: false

# Logging configuration
logging:
  use_wandb: false  # Set to true to enable Weights & Biases logging
  wandb_project: "whale-ltv-transformer"
  wandb_name: "whale-ltv-transformer-run"
  log_every_n_steps: 50

# Evaluation configuration
evaluation:
  run_baselines: true  # Run baseline model comparisons 